{"title":"chapter3","markdown":{"yaml":{"title":"chapter3"},"headingText":"Linear Regression","containsRefs":false,"markdown":"\n```{r setup, echo = FALSE}\nknitr::opts_chunk$set(error = TRUE)\n```\n\n\n\n## Libraries\n\nThe `library()` function is used to load *libraries*, or groups of  functions and data sets that are not included in the base `R` distribution. Basic functions that perform least squares linear regression and other simple analyses\ncome standard with the base distribution,  but more exotic functions require additional libraries.\n Here we load the `MASS` package, which is a very large collection of data sets and functions. We  also load the `ISLR2` package, which includes the data sets associated with this book.\n\n\n```{r chunk1}\nlibrary(MASS)\nlibrary(ISLR2)\n```\n\n\nIf you receive an error message when loading any of these libraries, it\nlikely indicates that the corresponding library has not yet been\ninstalled on your system. Some libraries, such as `MASS`, come with `R`\nand do not need to be separately installed on your computer. However,\nother packages, such as `ISLR2`, must be downloaded the first time they\nare used. This can be done directly from within `R`. For example, on a\nWindows system,  select the `Install package` option\nunder the `Packages` tab.  After you select any mirror site, a\nlist of available packages will appear. Simply select the package you\nwish to install and `R` will automatically download the\npackage. Alternatively, this can be done at the `R` command line\nvia `install.packages(\"ISLR2\")`. This installation only needs\nto be done the first time you use a package. However, the\n`library()` function must be called within each `R` session.\n\n## Simple Linear Regression\n\nThe `ISLR2` library contains the `Boston`  data set, which\nrecords `medv` (median house value) for $506$ census tracts in Boston. We will seek to predict `medv` using $12$ predictors such as `rmvar` (average number of  rooms per house), `age` (proportion of owner-occupied units built prior to 1940) and `lstat` (percent of households with low socioeconomic status).\n\n```{r chunk2}\nhead(Boston)\n```\n\nTo find out more about the data set, we can type `?Boston`.\n\nWe will start by using the `lm()` function to fit a simple  linear regression model, with `medv` as the response and `lstat`  as the predictor. The basic syntax is `lm(y ~ x, data)`, where `y` is the response, `x` is the predictor, and `data` is the data set in which these two variables are kept.\n\n```{r chunk3, error=TRUE}\nlm.fit <- lm(medv ~ lstat)\n```\n\nThe command causes an error because `R` does not know where to find the variables `medv` and `lstat`. The next line tells `R` that the variables are in `Boston`. If we attach `Boston`, the first line works fine because `R` now recognizes the variables.\n\n```{r chunk4}\nlm.fit <- lm(medv ~ lstat, data = Boston)\nattach(Boston)\nlm.fit <- lm(medv ~ lstat)\n```\n\n\nIf we type `lm.fit`,  some basic information about the model is output. For more detailed information, we use `summary(lm.fit)`. This gives us $p$-values and standard errors for the coefficients, as well as the $R^2$ statistic and $F$-statistic for the model.\n\n\n```{r chunk5}\nlm.fit\nsummary(lm.fit)\n```\n\nWe can use the `names()` function in order to find out what other pieces of information  are stored in `lm.fit`.\nAlthough we can extract these quantities by name---e.g. `lm.fit$coefficients`---it is safer to use the extractor functions like `coef()` to access them.\n\n```{r chunk6}\nnames(lm.fit)\ncoef(lm.fit)\n```\n\nIn order to obtain a confidence interval for the coefficient estimates, we can use the `confint()` command.\n\n```{r chunk7}\nconfint(lm.fit)\n```\n\nThe `predict()` function can be used to produce confidence intervals and prediction intervals for the prediction of `medv` for a given value of `lstat`.\n\n```{r chunk8}\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"confidence\")\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"prediction\")\n```\n\nFor instance, the 95 \\% confidence interval associated with a `lstat` value of 10 is $(24.47, 25.63)$, and the 95 \\% prediction interval is $(12.828, 37.28)$.\nAs expected, the confidence and prediction intervals are centered around the same point (a predicted value of $25.05$ for `medv` when `lstat` equals 10), but the latter are substantially wider.\n\nWe will now plot `medv` and `lstat` along with the least squares regression line using the `plot()` and `abline()` functions.\n\n```{r chunk9}\nplot(lstat, medv)\nabline(lm.fit)\n```\n\nThere is some evidence for non-linearity in the relationship between `lstat` and `medv`. We will explore this issue  later in this lab.\n\nThe `abline()` function can be used to draw any line, not just the least squares regression line.\nTo draw a line with intercept `a` and slope `b`, we  type `abline(a, b)`. Below we experiment with some additional settings for plotting lines and points.\n The `lwd = 3` command causes the width of the regression line to be increased by a factor of 3;  this works for the `plot()` and `lines()` functions also. We can also use the `pch` option to create different plotting symbols.\n\n```{r chunk10}\nplot(lstat, medv)\nabline(lm.fit, lwd = 3)\nabline(lm.fit, lwd = 3, col = \"red\")\nplot(lstat, medv, col = \"red\")\nplot(lstat, medv, pch = 20)\nplot(lstat, medv, pch = \"+\")\nplot(1:20, 1:20, pch = 1:20)\n```\n\n\nNext we examine some diagnostic plots, several of which were discussed\nin Section 3.3.3. Four diagnostic plots are automatically\nproduced by applying the `plot()` function directly to the output\nfrom `lm()`. In general, this command will produce one plot at a\ntime, and hitting *Enter* will generate the next plot. However,\nit is often convenient to view all four plots together. We can achieve\nthis by using the `par()` and `mfrow()` functions, which tell `R` to split\nthe display screen into separate panels so that multiple plots can be\nviewed simultaneously. For example,  `par(mfrow = c(2, 2))` divides the plotting\nregion into a $2 \\times 2$ grid of panels.\n\n```{r chunk11}\npar(mfrow = c(2, 2))\nplot(lm.fit)\n```\n\nAlternatively, we can compute the residuals from a linear regression\nfit using the `residuals()` function. The function\n`rstudent()` will return the studentized residuals, and we\ncan use this function to plot the residuals against the fitted values.\n\n```{r chunk12}\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\n```\n\nOn the basis of the residual plots, there is some evidence of non-linearity.\nLeverage statistics can be computed for any number of predictors using the `hatvalues()` function.\n\n```{r chunk13}\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n```\n\nThe `which.max()` function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.\n\n## Multiple Linear Regression\n\nIn order to fit a multiple linear regression model using least squares, we again use the `lm()` function. The syntax `lm(y ~ x1 + x2 + x3)` is used to fit a model with three predictors, `x1`, `x2`, and `x3`.\nThe `summary()` function now outputs the regression coefficients for all the predictors.\n\n```{r chunk14}\nlm.fit <- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n```\n\nThe `Boston` data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors.\nInstead, we can use the following short-hand:\n\n```{r chunk15}\nlm.fit <- lm(medv ~ ., data = Boston)\nsummary(lm.fit)\n```\n\nWe can access the individual components of a summary object by name\n(type `?summary.lm` to see what is available). Hence\n`summary(lm.fit)$r.sq` gives us the $R^2$, and\n`summary(lm.fit)$sigma` gives us the RSE. The `vif()`\nfunction, part of the `car` package, can be used to compute variance\ninflation factors.  Most VIF's are\nlow to moderate for this data. The `car` package is not part of the base `R` installation so it must be downloaded the first time you use it via the `install.packages()` function in `R`.\n\n```{r chunk16}\nlibrary(car)\nvif(lm.fit)\n```\n\nWhat if we would like to perform a regression using all of the variables but one?  For example, in the above regression output,  `age` has a high $p$-value. So we may wish to run a regression excluding this predictor.\n The following syntax results in a regression using all predictors except `age`.\n\n```{r chunk17}\nlm.fit1 <- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n```\n\nAlternatively, the `update()` function can be used.\n\n```{r chunk18}\nlm.fit1 <- update(lm.fit, ~ . - age)\n```\n\n\n## Interaction Terms\n\nIt is easy to include interaction terms in a linear model using the `lm()` function. The syntax `lstat:age` tells `R` to include an interaction term between `lstat` and `age`.\nThe syntax `lstat * age` simultaneously includes `lstat`, `age`, and the interaction term `lstat`$\\times$`age` as predictors; it is a shorthand for `lstat + age + lstat:age`.\n  %We can also pass in transformed versions of the predictors.\n\n```{r chunk19}\nsummary(lm(medv ~ lstat * age, data = Boston))\n```\n\n\n## Non-linear Transformations of the Predictors\n\nThe `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor $X$, we can create a predictor $X^2$ using\n `I(X^2)`. The function `I()` is needed since the `^` has a special meaning in a formula object; wrapping as we do allows the standard usage in `R`, which is to raise `X` to the power `2`. We now\nperform a regression of `medv` onto `lstat` and `lstat^2`.\n\n```{r chunk20}\nlm.fit2 <- lm(medv ~ lstat + I(lstat^2))\nsummary(lm.fit2)\n```\n\nThe near-zero $p$-value associated with the quadratic term suggests that it leads to an improved model.\nWe use the `anova()` function  to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\n```{r chunk21}\nlm.fit <- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\n```\n\nHere Model 1 represents the linear submodel containing only one predictor, `lstat`, while Model 2 corresponds to the larger quadratic model that has two predictors, `lstat` and `lstat^2`.\nThe `anova()` function performs a hypothesis test\ncomparing the two models. The   null hypothesis is that the two models fit the data equally well,  and the alternative hypothesis is that the full model is superior. Here the $F$-statistic is $135$\n and the associated $p$-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat^2` is far superior to the model that only contains the predictor `lstat`.\n This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. If we type\n\n```{r chunk22}\npar(mfrow = c(2, 2))\nplot(lm.fit2)\n```\n\n then we see that when the `lstat^2` term is included in the model, there is little discernible pattern in the residuals.\n\nIn order to create a cubic fit, we can include a predictor of the form `I(X^3)`. However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the `poly()` function to create the polynomial within `lm()`. For example, the following command produces a\nfifth-order polynomial fit:\n\n```{r chunk23}\nlm.fit5 <- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\n```\n\nThis suggests that including additional  polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant $p$-values\nin a regression fit.\n\n By default, the `poly()` function orthogonalizes the predictors:\n this means that the features output by this function are not simply a\n sequence of powers of the argument. However, a linear model applied to the output of the `poly()` function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the `poly()` function,  the argument `raw = TRUE` must be used.\n\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\n```{r chunk24}\nsummary(lm(medv ~ log(rm), data = Boston))\n```\n\n\n## Qualitative Predictors\n\nWe will now examine the `Carseats` data, which is part of the\n`ISLR2` library. We will  attempt to predict `Sales`\n(child car seat sales) in $400$ locations based on a number of\npredictors.\n\n```{r chunk25}\nhead(Carseats)\n```\n\nThe `Carseats` data includes qualitative predictors such as `shelveloc`, an indicator of the quality of the shelving location---that is, the  space within a store in which the car seat is displayed---at each location. The predictor `shelveloc` takes on three possible values:  *Bad*, *Medium*, and *Good*. Given a qualitative variable such as `shelveloc`, `R` generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\n```{r chunk26}\nlm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, \n    data = Carseats)\nsummary(lm.fit)\n```\n\nThe `contrasts()` function returns the coding that `R` uses for the dummy variables.\n\n```{r chunk27}\nattach(Carseats)\ncontrasts(ShelveLoc)\n```\n\nUse `?contrasts` to learn about other contrasts, and how to set them.\n\n`R` has created a `ShelveLocGood` dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a `ShelveLocMedium` dummy variable that equals 1 if the shelving location is medium, and 0 otherwise.\nA bad shelving location corresponds to a zero for each of the two dummy variables.\nThe fact that the coefficient for `ShelveLocGood` in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And `ShelveLocMedium` has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.\n\n\n## Writing  Functions\n\nAs we have seen, `R` comes with many useful functions, and still more functions are available by way of `R` libraries.\nHowever, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the `ISLR2` and `MASS` libraries, called\n`LoadLibraries()`. Before we have created the function, `R` returns an error if we try to call it.\n\n```{r chunk28, error=TRUE}\nLoadLibraries\nLoadLibraries()\n```\n\nWe now create the function. Note that the `+` symbols are printed by `R` and should not be typed in. The `{` symbol informs `R` that multiple commands are about to be input. Hitting *Enter* after typing `{` will cause `R` to print the `+` symbol. We can then input as many commands as we wish, hitting {*Enter*} after each one. Finally the `}` symbol informs `R` that no further commands will be entered.\n\n```{r chunk29}\nLoadLibraries <- function() {\n library(ISLR2)\n library(MASS)\n print(\"The libraries have been loaded.\")\n}\n```\n\nNow if we type in `LoadLibraries`, `R` will tell us what is in the function.\n\n```{r chunk30}\nLoadLibraries\n```\n\nIf we call the function, the libraries are loaded in and the print statement is output.\n\n```{r chunk31}\nLoadLibraries()\n```\n\n","srcMarkdownNoYaml":"\n```{r setup, echo = FALSE}\nknitr::opts_chunk$set(error = TRUE)\n```\n\n# Linear Regression\n\n\n## Libraries\n\nThe `library()` function is used to load *libraries*, or groups of  functions and data sets that are not included in the base `R` distribution. Basic functions that perform least squares linear regression and other simple analyses\ncome standard with the base distribution,  but more exotic functions require additional libraries.\n Here we load the `MASS` package, which is a very large collection of data sets and functions. We  also load the `ISLR2` package, which includes the data sets associated with this book.\n\n\n```{r chunk1}\nlibrary(MASS)\nlibrary(ISLR2)\n```\n\n\nIf you receive an error message when loading any of these libraries, it\nlikely indicates that the corresponding library has not yet been\ninstalled on your system. Some libraries, such as `MASS`, come with `R`\nand do not need to be separately installed on your computer. However,\nother packages, such as `ISLR2`, must be downloaded the first time they\nare used. This can be done directly from within `R`. For example, on a\nWindows system,  select the `Install package` option\nunder the `Packages` tab.  After you select any mirror site, a\nlist of available packages will appear. Simply select the package you\nwish to install and `R` will automatically download the\npackage. Alternatively, this can be done at the `R` command line\nvia `install.packages(\"ISLR2\")`. This installation only needs\nto be done the first time you use a package. However, the\n`library()` function must be called within each `R` session.\n\n## Simple Linear Regression\n\nThe `ISLR2` library contains the `Boston`  data set, which\nrecords `medv` (median house value) for $506$ census tracts in Boston. We will seek to predict `medv` using $12$ predictors such as `rmvar` (average number of  rooms per house), `age` (proportion of owner-occupied units built prior to 1940) and `lstat` (percent of households with low socioeconomic status).\n\n```{r chunk2}\nhead(Boston)\n```\n\nTo find out more about the data set, we can type `?Boston`.\n\nWe will start by using the `lm()` function to fit a simple  linear regression model, with `medv` as the response and `lstat`  as the predictor. The basic syntax is `lm(y ~ x, data)`, where `y` is the response, `x` is the predictor, and `data` is the data set in which these two variables are kept.\n\n```{r chunk3, error=TRUE}\nlm.fit <- lm(medv ~ lstat)\n```\n\nThe command causes an error because `R` does not know where to find the variables `medv` and `lstat`. The next line tells `R` that the variables are in `Boston`. If we attach `Boston`, the first line works fine because `R` now recognizes the variables.\n\n```{r chunk4}\nlm.fit <- lm(medv ~ lstat, data = Boston)\nattach(Boston)\nlm.fit <- lm(medv ~ lstat)\n```\n\n\nIf we type `lm.fit`,  some basic information about the model is output. For more detailed information, we use `summary(lm.fit)`. This gives us $p$-values and standard errors for the coefficients, as well as the $R^2$ statistic and $F$-statistic for the model.\n\n\n```{r chunk5}\nlm.fit\nsummary(lm.fit)\n```\n\nWe can use the `names()` function in order to find out what other pieces of information  are stored in `lm.fit`.\nAlthough we can extract these quantities by name---e.g. `lm.fit$coefficients`---it is safer to use the extractor functions like `coef()` to access them.\n\n```{r chunk6}\nnames(lm.fit)\ncoef(lm.fit)\n```\n\nIn order to obtain a confidence interval for the coefficient estimates, we can use the `confint()` command.\n\n```{r chunk7}\nconfint(lm.fit)\n```\n\nThe `predict()` function can be used to produce confidence intervals and prediction intervals for the prediction of `medv` for a given value of `lstat`.\n\n```{r chunk8}\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"confidence\")\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"prediction\")\n```\n\nFor instance, the 95 \\% confidence interval associated with a `lstat` value of 10 is $(24.47, 25.63)$, and the 95 \\% prediction interval is $(12.828, 37.28)$.\nAs expected, the confidence and prediction intervals are centered around the same point (a predicted value of $25.05$ for `medv` when `lstat` equals 10), but the latter are substantially wider.\n\nWe will now plot `medv` and `lstat` along with the least squares regression line using the `plot()` and `abline()` functions.\n\n```{r chunk9}\nplot(lstat, medv)\nabline(lm.fit)\n```\n\nThere is some evidence for non-linearity in the relationship between `lstat` and `medv`. We will explore this issue  later in this lab.\n\nThe `abline()` function can be used to draw any line, not just the least squares regression line.\nTo draw a line with intercept `a` and slope `b`, we  type `abline(a, b)`. Below we experiment with some additional settings for plotting lines and points.\n The `lwd = 3` command causes the width of the regression line to be increased by a factor of 3;  this works for the `plot()` and `lines()` functions also. We can also use the `pch` option to create different plotting symbols.\n\n```{r chunk10}\nplot(lstat, medv)\nabline(lm.fit, lwd = 3)\nabline(lm.fit, lwd = 3, col = \"red\")\nplot(lstat, medv, col = \"red\")\nplot(lstat, medv, pch = 20)\nplot(lstat, medv, pch = \"+\")\nplot(1:20, 1:20, pch = 1:20)\n```\n\n\nNext we examine some diagnostic plots, several of which were discussed\nin Section 3.3.3. Four diagnostic plots are automatically\nproduced by applying the `plot()` function directly to the output\nfrom `lm()`. In general, this command will produce one plot at a\ntime, and hitting *Enter* will generate the next plot. However,\nit is often convenient to view all four plots together. We can achieve\nthis by using the `par()` and `mfrow()` functions, which tell `R` to split\nthe display screen into separate panels so that multiple plots can be\nviewed simultaneously. For example,  `par(mfrow = c(2, 2))` divides the plotting\nregion into a $2 \\times 2$ grid of panels.\n\n```{r chunk11}\npar(mfrow = c(2, 2))\nplot(lm.fit)\n```\n\nAlternatively, we can compute the residuals from a linear regression\nfit using the `residuals()` function. The function\n`rstudent()` will return the studentized residuals, and we\ncan use this function to plot the residuals against the fitted values.\n\n```{r chunk12}\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\n```\n\nOn the basis of the residual plots, there is some evidence of non-linearity.\nLeverage statistics can be computed for any number of predictors using the `hatvalues()` function.\n\n```{r chunk13}\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n```\n\nThe `which.max()` function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.\n\n## Multiple Linear Regression\n\nIn order to fit a multiple linear regression model using least squares, we again use the `lm()` function. The syntax `lm(y ~ x1 + x2 + x3)` is used to fit a model with three predictors, `x1`, `x2`, and `x3`.\nThe `summary()` function now outputs the regression coefficients for all the predictors.\n\n```{r chunk14}\nlm.fit <- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n```\n\nThe `Boston` data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors.\nInstead, we can use the following short-hand:\n\n```{r chunk15}\nlm.fit <- lm(medv ~ ., data = Boston)\nsummary(lm.fit)\n```\n\nWe can access the individual components of a summary object by name\n(type `?summary.lm` to see what is available). Hence\n`summary(lm.fit)$r.sq` gives us the $R^2$, and\n`summary(lm.fit)$sigma` gives us the RSE. The `vif()`\nfunction, part of the `car` package, can be used to compute variance\ninflation factors.  Most VIF's are\nlow to moderate for this data. The `car` package is not part of the base `R` installation so it must be downloaded the first time you use it via the `install.packages()` function in `R`.\n\n```{r chunk16}\nlibrary(car)\nvif(lm.fit)\n```\n\nWhat if we would like to perform a regression using all of the variables but one?  For example, in the above regression output,  `age` has a high $p$-value. So we may wish to run a regression excluding this predictor.\n The following syntax results in a regression using all predictors except `age`.\n\n```{r chunk17}\nlm.fit1 <- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n```\n\nAlternatively, the `update()` function can be used.\n\n```{r chunk18}\nlm.fit1 <- update(lm.fit, ~ . - age)\n```\n\n\n## Interaction Terms\n\nIt is easy to include interaction terms in a linear model using the `lm()` function. The syntax `lstat:age` tells `R` to include an interaction term between `lstat` and `age`.\nThe syntax `lstat * age` simultaneously includes `lstat`, `age`, and the interaction term `lstat`$\\times$`age` as predictors; it is a shorthand for `lstat + age + lstat:age`.\n  %We can also pass in transformed versions of the predictors.\n\n```{r chunk19}\nsummary(lm(medv ~ lstat * age, data = Boston))\n```\n\n\n## Non-linear Transformations of the Predictors\n\nThe `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor $X$, we can create a predictor $X^2$ using\n `I(X^2)`. The function `I()` is needed since the `^` has a special meaning in a formula object; wrapping as we do allows the standard usage in `R`, which is to raise `X` to the power `2`. We now\nperform a regression of `medv` onto `lstat` and `lstat^2`.\n\n```{r chunk20}\nlm.fit2 <- lm(medv ~ lstat + I(lstat^2))\nsummary(lm.fit2)\n```\n\nThe near-zero $p$-value associated with the quadratic term suggests that it leads to an improved model.\nWe use the `anova()` function  to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\n```{r chunk21}\nlm.fit <- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\n```\n\nHere Model 1 represents the linear submodel containing only one predictor, `lstat`, while Model 2 corresponds to the larger quadratic model that has two predictors, `lstat` and `lstat^2`.\nThe `anova()` function performs a hypothesis test\ncomparing the two models. The   null hypothesis is that the two models fit the data equally well,  and the alternative hypothesis is that the full model is superior. Here the $F$-statistic is $135$\n and the associated $p$-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat^2` is far superior to the model that only contains the predictor `lstat`.\n This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. If we type\n\n```{r chunk22}\npar(mfrow = c(2, 2))\nplot(lm.fit2)\n```\n\n then we see that when the `lstat^2` term is included in the model, there is little discernible pattern in the residuals.\n\nIn order to create a cubic fit, we can include a predictor of the form `I(X^3)`. However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the `poly()` function to create the polynomial within `lm()`. For example, the following command produces a\nfifth-order polynomial fit:\n\n```{r chunk23}\nlm.fit5 <- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\n```\n\nThis suggests that including additional  polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant $p$-values\nin a regression fit.\n\n By default, the `poly()` function orthogonalizes the predictors:\n this means that the features output by this function are not simply a\n sequence of powers of the argument. However, a linear model applied to the output of the `poly()` function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the `poly()` function,  the argument `raw = TRUE` must be used.\n\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\n```{r chunk24}\nsummary(lm(medv ~ log(rm), data = Boston))\n```\n\n\n## Qualitative Predictors\n\nWe will now examine the `Carseats` data, which is part of the\n`ISLR2` library. We will  attempt to predict `Sales`\n(child car seat sales) in $400$ locations based on a number of\npredictors.\n\n```{r chunk25}\nhead(Carseats)\n```\n\nThe `Carseats` data includes qualitative predictors such as `shelveloc`, an indicator of the quality of the shelving location---that is, the  space within a store in which the car seat is displayed---at each location. The predictor `shelveloc` takes on three possible values:  *Bad*, *Medium*, and *Good*. Given a qualitative variable such as `shelveloc`, `R` generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\n```{r chunk26}\nlm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, \n    data = Carseats)\nsummary(lm.fit)\n```\n\nThe `contrasts()` function returns the coding that `R` uses for the dummy variables.\n\n```{r chunk27}\nattach(Carseats)\ncontrasts(ShelveLoc)\n```\n\nUse `?contrasts` to learn about other contrasts, and how to set them.\n\n`R` has created a `ShelveLocGood` dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a `ShelveLocMedium` dummy variable that equals 1 if the shelving location is medium, and 0 otherwise.\nA bad shelving location corresponds to a zero for each of the two dummy variables.\nThe fact that the coefficient for `ShelveLocGood` in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And `ShelveLocMedium` has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.\n\n\n## Writing  Functions\n\nAs we have seen, `R` comes with many useful functions, and still more functions are available by way of `R` libraries.\nHowever, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the `ISLR2` and `MASS` libraries, called\n`LoadLibraries()`. Before we have created the function, `R` returns an error if we try to call it.\n\n```{r chunk28, error=TRUE}\nLoadLibraries\nLoadLibraries()\n```\n\nWe now create the function. Note that the `+` symbols are printed by `R` and should not be typed in. The `{` symbol informs `R` that multiple commands are about to be input. Hitting *Enter* after typing `{` will cause `R` to print the `+` symbol. We can then input as many commands as we wish, hitting {*Enter*} after each one. Finally the `}` symbol informs `R` that no further commands will be entered.\n\n```{r chunk29}\nLoadLibraries <- function() {\n library(ISLR2)\n library(MASS)\n print(\"The libraries have been loaded.\")\n}\n```\n\nNow if we type in `LoadLibraries`, `R` will tell us what is in the function.\n\n```{r chunk30}\nLoadLibraries\n```\n\nIf we call the function, the libraries are loaded in and the print statement is output.\n\n```{r chunk31}\nLoadLibraries()\n```\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"chapter3.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.553","editor":"visual","theme":"cosmo","title":"chapter3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}